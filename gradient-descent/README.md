# Gradient Descent
Iterative first-order optimisation algorithm used to find a local minimum/maximum (minimizes a cost/loss function). If the function is strictly convex/concave, the minimum/maximum found will be global (if it exists, the exponential function is convex, but doesn't have a minimum). It requires the fuction to be differentiable (have a derivative for each point in the domain).

## How does it work?
Pandas are cool

## What happens when the function is not convex/concave?
A function is strightly convex if any straight line segment between two points lies above the graph. If it touches the graph in any point other than the two points, it is considered to be convex. If a function (f) is convex, its negative (-f) is concave.


## Sources
- https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21 
- https://realpython.com/gradient-descent-algorithm-python/ 
